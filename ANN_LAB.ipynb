{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hkOhYmMcnmZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnpRld9-cnma",
        "outputId": "0ef10a9c-9e77-4a12-ab37-34c65c72f189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard thereâ€™s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/twits.json'\n",
        "with open(file_path, 'r') as f:\n",
        "    twits = json.load(f)\n",
        "\n",
        "print(twits['data'][:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ZQnjwIcnmb"
      },
      "source": [
        "### Length of Data\n",
        "Now let's look at the number of twits in dataset. Print the number of twits below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxFxAz5Lcnmb",
        "outputId": "e326693c-d2ed-4f95-b30d-d8082f06b8d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1548010"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "len(twits['data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LZg05ZScnmc"
      },
      "source": [
        "### Split Message Body and Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETw977T7cnmc"
      },
      "outputs": [],
      "source": [
        "messages = [twit['message_body'] for twit in twits['data']]\n",
        "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIw-OUUWcnmc"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqcvhJpPcnmc",
        "outputId": "67b9defe-dfd7-4dd7-deb4-83a1ce68bee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def preprocess(message):\n",
        "\n",
        "\n",
        "    text = message.lower()\n",
        "\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
        "\n",
        "    text = re.sub(r'(\\$[a-zA-Z]*)', ' ', text, flags=re.MULTILINE)\n",
        "\n",
        "    text = re.sub(r'(@[a-zA-Z]*)', ' ', text, flags=re.MULTILINE)\n",
        "\n",
        "    text = re.sub(r'([^a-zA-Z])', ' ', text, flags=re.MULTILINE)\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    wnl = nltk.stem.WordNetLemmatizer()\n",
        "    tokens = [wnl.lemmatize(token) for token in tokens if len(token)>1]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFnoJK7Qcnmd"
      },
      "source": [
        "### Preprocess All the Twits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY7dh9DRcnmd"
      },
      "outputs": [],
      "source": [
        "tokenized = [preprocess(message) for message in messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OHjexyfcnmd"
      },
      "source": [
        "### Bag of Words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2ZFKyTJcnmd"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "bow = Counter(word for sentence in tokenized for word in sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQE2d9Eacnmd",
        "outputId": "8b71d2d1-bf7a-44b3-f963-02592f27aaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the', 397917), ('to', 378596), ('is', 283874), ('for', 272841), ('on', 241389), ('of', 210919), ('and', 208194), ('in', 204412), ('this', 203027), ('it', 193166)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4537"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_word_count = sum(bow.values())\n",
        "freqs = {word: count/total_word_count for word, count in bow.items()}\n",
        "\n",
        "low_cutoff = 1e-5\n",
        "\n",
        "high_cutoff = 10\n",
        "\n",
        "K_most_common = bow.most_common(high_cutoff)\n",
        "\n",
        "\n",
        "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
        "print(K_most_common)\n",
        "len(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_37Jvy8cnmd"
      },
      "source": [
        "### Updating Vocabulary by Removing Filtered Words\n",
        "Let's creat three variables that will help with our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOXfLbwucnmd"
      },
      "outputs": [],
      "source": [
        "vocab = {word: ii for ii, word in enumerate(filtered_words)}\n",
        "\n",
        "id2vocab = {ii: word for ii, word in vocab.items()}\n",
        "\n",
        "filtered = [[word for word in sentence if word in vocab] for sentence in tokenized]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eK8wjpTcnme"
      },
      "outputs": [],
      "source": [
        "balanced = {'messages': [], 'sentiments':[]}\n",
        "\n",
        "n_neutral = sum(1 for each in sentiments if each == 2)\n",
        "N_examples = len(sentiments)\n",
        "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
        "\n",
        "for idx, sentiment in enumerate(sentiments):\n",
        "    message = filtered[idx]\n",
        "    if len(message) == 0:\n",
        "        continue\n",
        "    elif sentiment != 2 or random.random() < keep_prob:\n",
        "        balanced['messages'].append(message)\n",
        "        balanced['sentiments'].append(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrT6gJgecnme",
        "outputId": "fad22605-0be0-4ae1-9b8c-24094c26ed03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1941367125624467"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
        "N_examples = len(balanced['sentiments'])\n",
        "n_neutral/N_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0swAuLIcnme"
      },
      "outputs": [],
      "source": [
        "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
        "sentiments = balanced['sentiments']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPF6MeRcnme"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "\n",
        "#### Embed -> RNN -> Dense -> Softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcuDNj13cnme"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.output_size = output_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
        "        self.dense = nn.Linear(in_features=lstm_size, out_features=output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden_state = weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_()\n",
        "        cell_state = weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_()\n",
        "\n",
        "        return (hidden_state, cell_state)\n",
        "\n",
        "\n",
        "    def forward(self, nn_input, hidden_state):\n",
        "\n",
        "        batch_size = nn_input.size(0)\n",
        "\n",
        "        x = nn_input.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden_state)\n",
        "\n",
        "        lstm_out = lstm_out[-1,:,:]\n",
        "\n",
        "        dense_out = self.dense(lstm_out)\n",
        "\n",
        "        softmax_out = self.softmax(dense_out)\n",
        "\n",
        "        return softmax_out, hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gdgg9Xwcnmf"
      },
      "source": [
        "### View Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvlloaB1cnmf",
        "outputId": "36336dfe-cfcf-4e56-808d-f59a5642daa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-1.9205, -1.3536, -1.6702, -1.5217, -1.6681],\n",
            "        [-1.9089, -1.3420, -1.6779, -1.5610, -1.6412],\n",
            "        [-1.9037, -1.3519, -1.6702, -1.5456, -1.6562],\n",
            "        [-1.9149, -1.3474, -1.6708, -1.5259, -1.6756]])\n"
          ]
        }
      ],
      "source": [
        "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
        "model.embedding.weight.data.uniform_(-1, 1)\n",
        "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
        "hidden = model.init_hidden(4)\n",
        "\n",
        "logps, _ = model.forward(input, hidden)\n",
        "print(logps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9BpGPE4cnmf"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqD7NXU0cnmf"
      },
      "outputs": [],
      "source": [
        "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
        "\n",
        "    if shuffle:\n",
        "        indices = list(range(len(messages)))\n",
        "        random.shuffle(indices)\n",
        "        messages = [messages[idx] for idx in indices]\n",
        "        labels = [labels[idx] for idx in indices]\n",
        "\n",
        "    total_sequences = len(messages)\n",
        "\n",
        "    for ii in range(0, total_sequences, batch_size):\n",
        "        batch_messages = messages[ii: ii+batch_size]\n",
        "\n",
        "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
        "        for batch_num, tokens in enumerate(batch_messages):\n",
        "            token_tensor = torch.tensor(tokens)\n",
        "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
        "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
        "\n",
        "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
        "\n",
        "        yield batch, label_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfUqwkCPcnmf"
      },
      "source": [
        "### Training and  Validation\n",
        "With our data in nice shape, we'll split it into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR5XngS1cnmf"
      },
      "outputs": [],
      "source": [
        "def train_test_split(dataset, split=0.70):\n",
        "    split_idx = int(len(dataset) * split)\n",
        "\n",
        "    return dataset[:split_idx], dataset[split_idx:]\n",
        "\n",
        "train_features, valid_features = train_test_split(token_ids)\n",
        "train_labels, valid_labels = train_test_split(sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_qjupzscnmg"
      },
      "outputs": [],
      "source": [
        "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
        "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
        "hidden = model.init_hidden(64)\n",
        "logps, hidden = model.forward(text_batch, hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOfJcAptcnmg"
      },
      "source": [
        "### Training\n",
        "It's time to train the neural network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aFhynZ0cnmg",
        "outputId": "bc8b8a1b-eaaa-4d8d-8b81-e6fa26916f20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextClassifier(\n",
              "  (embedding): Embedding(4538, 1024, padding_idx=0)\n",
              "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
              "  (dense): Linear(in_features=512, out_features=5, bias=True)\n",
              "  (softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
        "model.embedding.weight.data.uniform_(-1, 1)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bRW0oKnUcnmg",
        "outputId": "c605f5e5-2865-4a34-d027-91ede6256009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Epoch: 1/5... Step: 100... Loss: 0.963563... Val Loss: 1.007029\n",
            "Epoch: 1/5... Step: 200... Loss: 0.836163... Val Loss: 0.907048\n",
            "Epoch: 1/5... Step: 300... Loss: 0.902836... Val Loss: 0.850816\n",
            "Epoch: 1/5... Step: 400... Loss: 0.789343... Val Loss: 0.809933\n",
            "Epoch: 1/5... Step: 500... Loss: 0.762552... Val Loss: 0.786965\n",
            "Epoch: 1/5... Step: 600... Loss: 0.687571... Val Loss: 0.769912\n",
            "Epoch: 1/5... Step: 700... Loss: 0.698573... Val Loss: 0.761188\n",
            "Epoch: 1/5... Step: 800... Loss: 0.816043... Val Loss: 0.743132\n",
            "Epoch: 1/5... Step: 900... Loss: 0.745562... Val Loss: 0.740738\n",
            "Epoch: 1/5... Step: 1000... Loss: 0.674274... Val Loss: 0.735963\n",
            "Epoch: 1/5... Step: 1100... Loss: 0.715941... Val Loss: 0.728147\n",
            "Epoch: 1/5... Step: 1200... Loss: 0.694073... Val Loss: 0.721874\n",
            "Epoch: 1/5... Step: 1300... Loss: 0.666030... Val Loss: 0.718588\n",
            "Epoch: 1/5... Step: 1400... Loss: 0.621976... Val Loss: 0.711632\n",
            "Starting epoch 2\n",
            "Epoch: 2/5... Step: 100... Loss: 0.634024... Val Loss: 0.717977\n",
            "Epoch: 2/5... Step: 200... Loss: 0.601061... Val Loss: 0.718184\n",
            "Epoch: 2/5... Step: 300... Loss: 0.601647... Val Loss: 0.713721\n",
            "Epoch: 2/5... Step: 400... Loss: 0.612313... Val Loss: 0.714405\n",
            "Epoch: 2/5... Step: 500... Loss: 0.643033... Val Loss: 0.709229\n",
            "Epoch: 2/5... Step: 600... Loss: 0.652192... Val Loss: 0.711832\n",
            "Epoch: 2/5... Step: 700... Loss: 0.720097... Val Loss: 0.712295\n",
            "Epoch: 2/5... Step: 800... Loss: 0.636695... Val Loss: 0.704938\n",
            "Epoch: 2/5... Step: 900... Loss: 0.648814... Val Loss: 0.703272\n",
            "Epoch: 2/5... Step: 1000... Loss: 0.648409... Val Loss: 0.703710\n",
            "Epoch: 2/5... Step: 1100... Loss: 0.724037... Val Loss: 0.703646\n",
            "Epoch: 2/5... Step: 1200... Loss: 0.644142... Val Loss: 0.699589\n",
            "Epoch: 2/5... Step: 1300... Loss: 0.663014... Val Loss: 0.699174\n",
            "Epoch: 2/5... Step: 1400... Loss: 0.622468... Val Loss: 0.697240\n",
            "Starting epoch 3\n",
            "Epoch: 3/5... Step: 100... Loss: 0.639669... Val Loss: 0.717965\n",
            "Epoch: 3/5... Step: 200... Loss: 0.580149... Val Loss: 0.715037\n",
            "Epoch: 3/5... Step: 300... Loss: 0.607766... Val Loss: 0.719095\n",
            "Epoch: 3/5... Step: 400... Loss: 0.556704... Val Loss: 0.716263\n",
            "Epoch: 3/5... Step: 500... Loss: 0.545675... Val Loss: 0.717309\n",
            "Epoch: 3/5... Step: 600... Loss: 0.581000... Val Loss: 0.710167\n",
            "Epoch: 3/5... Step: 700... Loss: 0.502769... Val Loss: 0.712914\n",
            "Epoch: 3/5... Step: 800... Loss: 0.627043... Val Loss: 0.709536\n",
            "Epoch: 3/5... Step: 900... Loss: 0.642484... Val Loss: 0.709652\n",
            "Epoch: 3/5... Step: 1000... Loss: 0.540881... Val Loss: 0.713452\n",
            "Epoch: 3/5... Step: 1100... Loss: 0.583765... Val Loss: 0.707245\n",
            "Epoch: 3/5... Step: 1200... Loss: 0.603871... Val Loss: 0.709636\n",
            "Epoch: 3/5... Step: 1300... Loss: 0.611160... Val Loss: 0.703078\n",
            "Epoch: 3/5... Step: 1400... Loss: 0.607357... Val Loss: 0.705525\n",
            "Starting epoch 4\n",
            "Epoch: 4/5... Step: 100... Loss: 0.460026... Val Loss: 0.749481\n",
            "Epoch: 4/5... Step: 200... Loss: 0.505381... Val Loss: 0.754213\n",
            "Epoch: 4/5... Step: 300... Loss: 0.578885... Val Loss: 0.749973\n",
            "Epoch: 4/5... Step: 400... Loss: 0.524700... Val Loss: 0.746054\n",
            "Epoch: 4/5... Step: 500... Loss: 0.545010... Val Loss: 0.749976\n",
            "Epoch: 4/5... Step: 600... Loss: 0.537783... Val Loss: 0.746726\n",
            "Epoch: 4/5... Step: 700... Loss: 0.539081... Val Loss: 0.746262\n",
            "Epoch: 4/5... Step: 800... Loss: 0.501376... Val Loss: 0.753877\n",
            "Epoch: 4/5... Step: 900... Loss: 0.546724... Val Loss: 0.748695\n",
            "Epoch: 4/5... Step: 1000... Loss: 0.572143... Val Loss: 0.745667\n",
            "Epoch: 4/5... Step: 1100... Loss: 0.492524... Val Loss: 0.739580\n",
            "Epoch: 4/5... Step: 1200... Loss: 0.614819... Val Loss: 0.739170\n",
            "Epoch: 4/5... Step: 1300... Loss: 0.552636... Val Loss: 0.733171\n",
            "Epoch: 4/5... Step: 1400... Loss: 0.523762... Val Loss: 0.735419\n",
            "Starting epoch 5\n",
            "Epoch: 5/5... Step: 100... Loss: 0.459271... Val Loss: 0.813114\n",
            "Epoch: 5/5... Step: 200... Loss: 0.429414... Val Loss: 0.819741\n",
            "Epoch: 5/5... Step: 300... Loss: 0.440429... Val Loss: 0.823253\n",
            "Epoch: 5/5... Step: 400... Loss: 0.414364... Val Loss: 0.825497\n",
            "Epoch: 5/5... Step: 500... Loss: 0.417894... Val Loss: 0.820151\n",
            "Epoch: 5/5... Step: 600... Loss: 0.422993... Val Loss: 0.819411\n",
            "Epoch: 5/5... Step: 700... Loss: 0.510148... Val Loss: 0.822174\n",
            "Epoch: 5/5... Step: 800... Loss: 0.460739... Val Loss: 0.804781\n",
            "Epoch: 5/5... Step: 900... Loss: 0.439372... Val Loss: 0.813354\n",
            "Epoch: 5/5... Step: 1000... Loss: 0.485543... Val Loss: 0.820711\n",
            "Epoch: 5/5... Step: 1100... Loss: 0.497685... Val Loss: 0.807576\n",
            "Epoch: 5/5... Step: 1200... Loss: 0.463493... Val Loss: 0.813669\n",
            "Epoch: 5/5... Step: 1300... Loss: 0.560916... Val Loss: 0.806058\n",
            "Epoch: 5/5... Step: 1400... Loss: 0.539898... Val Loss: 0.802428\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "batch_size = 512\n",
        "learning_rate = 0.001\n",
        "\n",
        "print_every = 100\n",
        "clip=5\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Starting epoch {}'.format(epoch + 1))\n",
        "\n",
        "    steps = 0\n",
        "    for text_batch, labels in dataloader(\n",
        "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
        "        steps += 1\n",
        "\n",
        "        hidden = model.init_hidden(labels.shape[0])\n",
        "\n",
        "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
        "        for each in hidden:\n",
        "            each.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs, hidden = model(text_batch, hidden)\n",
        "\n",
        "        loss = criterion(log_probs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % print_every == 0:\n",
        "            model.eval()\n",
        "\n",
        "            val_losses = []\n",
        "\n",
        "            for text_batch, labels in  dataloader(\n",
        "                valid_features, valid_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
        "\n",
        "                val_hidden = model.init_hidden(labels.shape[0])\n",
        "\n",
        "                text_batch, labels = text_batch.to(device), labels.to(device)\n",
        "                for each in val_hidden:\n",
        "                    each.to(device)\n",
        "\n",
        "                log_probs, val_hidden = model(text_batch, val_hidden)\n",
        "                val_loss = criterion(log_probs, labels)\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
        "                  \"Step: {}...\".format(steps),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2PiOVI_cnmg"
      },
      "source": [
        "## Making Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZRRMiYcnmg"
      },
      "outputs": [],
      "source": [
        "def predict(text, model, vocab):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tokens = preprocess(text)\n",
        "\n",
        "    tokens = [token for token in tokens if token in vocab]\n",
        "\n",
        "    tokens = [vocab[token] for token in tokens]\n",
        "\n",
        "    text_input = torch.from_numpy(np.asarray(torch.FloatTensor(tokens).view(-1,1)))\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    logps, _ = model.forward(text_input, hidden)\n",
        "\n",
        "    pred = torch.exp(logps)\n",
        "\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fqkL09vCdIBu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0N2cwVHcnmh"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
        "    test_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-fE9Ytycnmh"
      },
      "source": [
        "### Twit Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwH249hucnmi",
        "outputId": "ce43ce3d-1ddb-4f6c-8e23-ced355707c62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
              " 'timestamp': '2018-11-01T00:00:05Z'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def twit_stream():\n",
        "    for twit in test_data['data']:\n",
        "        yield twit\n",
        "\n",
        "next(twit_stream())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_1ZrYv7cnmi"
      },
      "source": [
        "Using the `prediction` function, let's apply it to a stream of twits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jI6GXXPcnmi"
      },
      "outputs": [],
      "source": [
        "def score_twits(stream, model, vocab, universe):\n",
        "\n",
        "    for twit in stream:\n",
        "\n",
        "        text = twit['message_body']\n",
        "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
        "        score = predict(text, model, vocab)\n",
        "\n",
        "        for symbol in symbols:\n",
        "            if symbol in universe:\n",
        "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "y37N0hBwcnmi",
        "outputId": "dc33fb01-4e2a-4172-ea9f-11a6fa10aa67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'symbol': '$AAPL',\n",
              " 'score': tensor([[ 0.1018,  0.0793,  0.1335,  0.1847,  0.5007]]),\n",
              " 'timestamp': '2018-11-01T00:00:18Z'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
        "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
        "\n",
        "next(score_stream)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}